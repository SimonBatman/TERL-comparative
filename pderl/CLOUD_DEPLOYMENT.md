# PDERL äº‘å¹³å°éƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨Linuxäº‘å¹³å°ä¸Šéƒ¨ç½²å’Œè¿è¡ŒPDERLé¡¹ç›®ï¼Œå……åˆ†åˆ©ç”¨GPUèµ„æºè¿›è¡Œå¹¶è¡Œè®­ç»ƒã€‚

## ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 18.04+ æ¨è)
- **GPU**: NVIDIA GPU with 16GB+ VRAM
- **CUDA**: 11.0+
- **Python**: 3.7+
- **å†…å­˜**: 32GB+ RAM æ¨è

## éƒ¨ç½²æ­¥éª¤

### 1. å…‹éš†é¡¹ç›®

```bash
# å…‹éš†é¡¹ç›®åˆ°äº‘å¹³å°
git clone https://github.com/your-username/TERL-comparative.git
cd TERL-comparative/pderl
```

### 2. ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n erl_env python=3.8
conda activate erl_env

# å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt

# å®‰è£…MuJoCoå’Œgymç¯å¢ƒ
pip install mujoco-py
pip install gym[mujoco]
```

### 3. éªŒè¯GPUç¯å¢ƒ

```bash
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
python -c "import torch; print('GPU count:', torch.cuda.device_count())"
python -c "import torch; print('GPU name:', torch.cuda.get_device_name(0))"
```

### 4. è¿è¡Œè®­ç»ƒ

#### ä½¿ç”¨äº¤äº’å¼è„šæœ¬

```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x run_parallel_experiments.sh

# è¿è¡Œäº¤äº’å¼è®­ç»ƒè„šæœ¬
./run_parallel_experiments.sh
```

#### ç›´æ¥å‘½ä»¤è¡Œè¿è¡Œ

```bash
# GPUä¼˜åŒ–é¢„è®¾ (æ¨èç”¨äº16Gæ˜¾å­˜ï¼ŒåŒ…å«TensorBoard)
python parallel_train.py -env Walker2d-v2 -preset gpu_optimized -workers 5

# å¯ç”¨TensorBoardçš„è‡ªå®šä¹‰è®­ç»ƒ
python parallel_train.py -env HalfCheetah-v2 -seeds 1 2 3 4 5 -workers 5 -use_cuda -use_tensorboard -log_weights

# å¿«é€Ÿæµ‹è¯•
python parallel_train.py -env Hopper-v2 -preset quick_test -workers 3

# ä¼ ç»ŸCSVè®°å½•æ–¹å¼
python parallel_train.py -env HalfCheetah-v2 -seeds 1 2 3 4 5 -workers 5 -use_cuda
```

### 5. TensorBoardç›‘æ§

```bash
# å¯åŠ¨TensorBoardæœåŠ¡
tensorboard --logdir=parallel_experiments --port=6006 --host=0.0.0.0

# åœ¨æµè§ˆå™¨ä¸­è®¿é—®
# http://your-server-ip:6006
```

## ğŸ“Š TensorBoardä½¿ç”¨æŒ‡å—

### å¯ç”¨TensorBoardè®°å½•
```bash
# æ–¹æ³•1: ä½¿ç”¨gpu_optimizedé¢„è®¾ (é»˜è®¤å¯ç”¨)
python parallel_train.py -env Hopper-v2 -preset gpu_optimized -workers 5

# æ–¹æ³•2: æ‰‹åŠ¨å¯ç”¨TensorBoard
python parallel_train.py -env Hopper-v2 -seeds 1 2 3 4 5 -use_tensorboard -log_weights -workers 5
```

### å¯åŠ¨TensorBoardæœåŠ¡
```bash
# åœ¨è®­ç»ƒç›®å½•å¯åŠ¨TensorBoard
tensorboard --logdir=parallel_experiments --port=6006 --host=0.0.0.0

# åå°è¿è¡Œ
nohup tensorboard --logdir=parallel_experiments --port=6006 --host=0.0.0.0 > tensorboard.log 2>&1 &
```

### è®¿é—®TensorBoard
- æœ¬åœ°è®¿é—®: `http://localhost:6006`
- è¿œç¨‹è®¿é—®: `http://your-server-ip:6006`
- äº‘å¹³å°: éœ€è¦å¼€æ”¾6006ç«¯å£

### TensorBoardè®°å½•å†…å®¹
- **æ€§èƒ½æŒ‡æ ‡**: ERLåˆ†æ•°ã€DDPGå¥–åŠ±ã€æœ€ä½³è®­ç»ƒé€‚åº”åº¦
- **æŸå¤±å‡½æ•°**: ç­–ç•¥æ¢¯åº¦æŸå¤±ã€è¡Œä¸ºå…‹éš†æŸå¤±
- **è¿›åŒ–ç»Ÿè®¡**: ç²¾è‹±æ¯”ä¾‹ã€é€‰æ‹©æ¯”ä¾‹ã€ä¸¢å¼ƒæ¯”ä¾‹ã€ç§ç¾¤æ–°é¢–æ€§
- **è®­ç»ƒè¿›åº¦**: æ¸¸æˆæ•°é‡ã€è®­ç»ƒæ—¶é—´
- **ç½‘ç»œæƒé‡**: Actorå’ŒCriticç½‘ç»œæƒé‡åˆ†å¸ƒ (å¯é€‰)
- **è‡ªå®šä¹‰æŒ‡æ ‡**: è®­ç»ƒæ•ˆç‡ã€èµ„æºä½¿ç”¨ç­‰

## ğŸ“Š é¢„è®¾é…ç½®è¯´æ˜

| é¢„è®¾åç§° | æè¿° | ç§å­æ•°é‡ | è®­ç»ƒå¸§æ•° | TensorBoard | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|---------|-------------|----------|
| `quick_test` | å¿«é€Ÿæµ‹è¯• | 3 | 50,000 | âŒ | éªŒè¯ç¯å¢ƒé…ç½® |
| `standard` | æ ‡å‡†å®éªŒ | 5 | é»˜è®¤ | âŒ | å¸¸è§„è®­ç»ƒ |
| `comprehensive` | å…¨é¢å®éªŒ | 10 | é»˜è®¤ | âŒ | å®Œæ•´è¯„ä¼° |
| `gpu_optimized` | GPUä¼˜åŒ– | 5 | 1,000,000 | âœ… | 16Gæ˜¾å­˜äº‘å¹³å° |
| `custom_seeds` | è‡ªå®šä¹‰ç§å­ | 5 | é»˜è®¤ | âŒ | ç‰¹å®šç§å­å®éªŒ |

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### GPUå†…å­˜ä¼˜åŒ–

- **16Gæ˜¾å­˜**: å»ºè®®æœ€å¤§å¹¶å‘æ•°ä¸º5
- **24Gæ˜¾å­˜**: å»ºè®®æœ€å¤§å¹¶å‘æ•°ä¸º7-8
- **32Gæ˜¾å­˜**: å»ºè®®æœ€å¤§å¹¶å‘æ•°ä¸º10+

### ç³»ç»Ÿèµ„æºç›‘æ§

```bash
# ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi

# ç›‘æ§ç³»ç»Ÿèµ„æº
htop

# ç›‘æ§ç£ç›˜ç©ºé—´
df -h
```

### é•¿æ—¶é—´è®­ç»ƒå»ºè®®

```bash
# ä½¿ç”¨screenæˆ–tmuxä¿æŒä¼šè¯
screen -S pderl_training
./run_parallel_experiments.sh
# Ctrl+A+D åˆ†ç¦»ä¼šè¯

# é‡æ–°è¿æ¥ä¼šè¯
screen -r pderl_training
```

## ç»“æœç®¡ç†

### è®­ç»ƒç»“æœç›®å½•ç»“æ„

```
parallel_experiments/
â”œâ”€â”€ experiment_report.json          # å®éªŒæ€»ç»“æŠ¥å‘Š
â”œâ”€â”€ Walker2d-v2_seed_1/
â”‚   â”œâ”€â”€ training.log                # è®­ç»ƒæ—¥å¿—
â”‚   â”œâ”€â”€ info.txt                    # å®éªŒä¿¡æ¯
â”‚   â”œâ”€â”€ tensorboard/                # TensorBoardæ—¥å¿— (å¦‚æœå¯ç”¨)
â”‚   â”‚   â””â”€â”€ events.out.tfevents.*   # TensorBoardäº‹ä»¶æ–‡ä»¶
â”‚   â”œâ”€â”€ erl_score.csv              # ERLåˆ†æ•°è®°å½• (å‘åå…¼å®¹)
â”‚   â”œâ”€â”€ ddpg_score.csv              # DDPGåˆ†æ•°è®°å½• (å‘åå…¼å®¹)
â”‚   â”œâ”€â”€ evo_net.pkl                 # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ models/                     # å‘¨æœŸæ€§ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ Walker2d-v2_seed_2/
â”‚   â””â”€â”€ ...
â””â”€â”€ Walker2d-v2_seed_3/
    â””â”€â”€ ...
```

### ç»“æœåˆ†æ

```bash
# åˆ†æå¹¶è¡Œè®­ç»ƒç»“æœ
python analyze_parallel_results.py

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
python visualize_results.py
```

### æ¨¡å‹æµ‹è¯•

```bash
# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
python test_trained_model.py -model parallel_experiments/Walker2d-v2_seed_1/model_best.pkl -env Walker2d-v2
```

## å¸¸è§é—®é¢˜

### 1. CUDAå†…å­˜ä¸è¶³

```bash
# å‡å°‘å¹¶å‘æ•°
python parallel_train.py -env Walker2d-v2 -preset gpu_optimized -workers 3

# æˆ–å‡å°‘ç§ç¾¤å¤§å°
python parallel_train.py -env Walker2d-v2 -seeds 1 2 3 -popsize 5 -workers 3
```

### 2. MuJoCoè®¸å¯è¯é—®é¢˜

```bash
# è®¾ç½®MuJoCoè®¸å¯è¯è·¯å¾„
export MUJOCO_PY_MUJOCO_PATH=/path/to/mujoco
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/mujoco/bin
```

### 3. è®­ç»ƒä¸­æ–­æ¢å¤

```bash
# æ£€æŸ¥ç°æœ‰å®éªŒ
ls parallel_experiments/

# ä»ç‰¹å®šæ£€æŸ¥ç‚¹æ¢å¤ (å¦‚æœæ”¯æŒ)
python run_pderl.py -env Walker2d-v2 -seed 1 -logdir parallel_experiments/Walker2d-v2_seed_1 -resume
```

## æ€§èƒ½åŸºå‡†

åœ¨é…ç½®ä¸º16G GPUå†…å­˜çš„äº‘å¹³å°ä¸Šï¼Œé¢„æœŸæ€§èƒ½ï¼š

- **Walker2d-v2**: ~2-3å°æ—¶/å®éªŒ (100ä¸‡å¸§)
- **HalfCheetah-v2**: ~2-4å°æ—¶/å®éªŒ
- **Ant-v2**: ~3-5å°æ—¶/å®éªŒ
- **å¹¶å‘5ä¸ªå®éªŒ**: æ€»æ—¶é—´çº¦3-6å°æ—¶

## æˆæœ¬ä¼˜åŒ–

1. **ä½¿ç”¨æŠ¢å å¼å®ä¾‹**: æˆæœ¬å¯é™ä½60-80%
2. **åˆç†å®‰æ’è®­ç»ƒæ—¶é—´**: é¿å¼€é«˜å³°æœŸ
3. **åŠæ—¶é‡Šæ”¾èµ„æº**: è®­ç»ƒå®Œæˆåç«‹å³åœæ­¢å®ä¾‹
4. **æ‰¹é‡å®éªŒ**: ä¸€æ¬¡æ€§è¿è¡Œå¤šä¸ªç¯å¢ƒçš„å®éªŒ

## æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. è®­ç»ƒæ—¥å¿—: `parallel_experiments/*/training.log`
2. ç³»ç»Ÿèµ„æº: `nvidia-smi`, `htop`
3. ç¯å¢ƒé…ç½®: `conda list`, `pip list`
4. CUDAç‰ˆæœ¬: `nvcc --version`

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£å’Œå¯è§†åŒ–æŒ‡å—ã€‚