#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDERL å¹¶è¡Œå®éªŒç»“æœåˆ†æè„šæœ¬
ç”¨äºåˆ†æå¤šä¸ªéšæœºç§å­å®éªŒçš„ç»“æœï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
"""

import os
import json
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ParallelResultsAnalyzer:
    def __init__(self, base_dir):
        self.base_dir = Path(base_dir)
        self.experiments = {}
        self.combined_data = None
        
    def load_experiment_data(self):
        """åŠ è½½æ‰€æœ‰å®éªŒæ•°æ®"""
        print(f"ğŸ” æ‰«æå®éªŒç›®å½•: {self.base_dir}")
        
        if not self.base_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.base_dir}")
            return False
        
        # æŸ¥æ‰¾æ‰€æœ‰å®éªŒå­ç›®å½•
        exp_dirs = [d for d in self.base_dir.iterdir() if d.is_dir() and 'seed_' in d.name]
        
        if not exp_dirs:
            print("âŒ æœªæ‰¾åˆ°å®éªŒç›®å½•")
            return False
        
        print(f"ğŸ“ æ‰¾åˆ° {len(exp_dirs)} ä¸ªå®éªŒç›®å½•")
        
        for exp_dir in exp_dirs:
            exp_name = exp_dir.name
            print(f"ğŸ“Š åŠ è½½å®éªŒ: {exp_name}")
            
            # è§£æå®éªŒåç§°
            parts = exp_name.split('_seed_')
            if len(parts) == 2:
                env_name = parts[0]
                seed = int(parts[1])
            else:
                print(f"âš ï¸ æ— æ³•è§£æå®éªŒåç§°: {exp_name}")
                continue
            
            # åŠ è½½å®éªŒæ•°æ®
            exp_data = self.load_single_experiment(exp_dir, env_name, seed)
            if exp_data:
                self.experiments[exp_name] = exp_data
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.experiments)} ä¸ªå®éªŒ")
        return len(self.experiments) > 0
    
    def load_single_experiment(self, exp_dir, env_name, seed):
        """åŠ è½½å•ä¸ªå®éªŒçš„æ•°æ®"""
        try:
            # æŸ¥æ‰¾CSVæ–‡ä»¶
            csv_files = {
                'erl_score': exp_dir / 'erl_score.csv',
                'ddpg_score': exp_dir / 'ddpg_score.csv',
                'frame_erl_score': exp_dir / 'frame_erl_score.csv',
                'time_erl_score': exp_dir / 'time_erl_score.csv'
            }
            
            data = {
                'env_name': env_name,
                'seed': seed,
                'directory': str(exp_dir)
            }
            
            # åŠ è½½å„ç§åˆ†æ•°æ•°æ®
            for score_type, csv_path in csv_files.items():
                if csv_path.exists():
                    try:
                        df = pd.read_csv(csv_path)
                        if not df.empty:
                            data[score_type] = df
                            # è®¡ç®—æœ€ç»ˆæ€§èƒ½
                            if 'fitness' in df.columns:
                                data[f'{score_type}_final'] = df['fitness'].iloc[-1]
                                data[f'{score_type}_max'] = df['fitness'].max()
                                data[f'{score_type}_mean'] = df['fitness'].mean()
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½ {csv_path} å¤±è´¥: {str(e)}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            model_files = list(exp_dir.glob('*.pkl'))
            data['model_files'] = [str(f) for f in model_files]
            data['has_models'] = len(model_files) > 0
            
            return data
            
        except Exception as e:
            print(f"âŒ åŠ è½½å®éªŒ {exp_dir.name} å¤±è´¥: {str(e)}")
            return None
    
    def create_combined_dataframe(self):
        """åˆ›å»ºåˆå¹¶çš„æ•°æ®æ¡†ç”¨äºåˆ†æ"""
        if not self.experiments:
            return None
        
        records = []
        
        for exp_name, exp_data in self.experiments.items():
            record = {
                'experiment': exp_name,
                'environment': exp_data['env_name'],
                'seed': exp_data['seed'],
                'has_models': exp_data['has_models']
            }
            
            # æ·»åŠ å„ç§åˆ†æ•°çš„æœ€ç»ˆå€¼
            score_types = ['erl_score', 'ddpg_score']
            for score_type in score_types:
                if f'{score_type}_final' in exp_data:
                    record[f'{score_type}_final'] = exp_data[f'{score_type}_final']
                    record[f'{score_type}_max'] = exp_data[f'{score_type}_max']
                    record[f'{score_type}_mean'] = exp_data[f'{score_type}_mean']
            
            records.append(record)
        
        self.combined_data = pd.DataFrame(records)
        return self.combined_data
    
    def generate_statistics_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if self.combined_data is None:
            self.create_combined_dataframe()
        
        if self.combined_data is None or self.combined_data.empty:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œåˆ†æ")
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        envs = self.combined_data['environment'].unique()
        print(f"ğŸ® ç¯å¢ƒæ•°é‡: {len(envs)}")
        for env in envs:
            env_data = self.combined_data[self.combined_data['environment'] == env]
            print(f"  ğŸ“‚ {env}: {len(env_data)} ä¸ªå®éªŒ")
        
        print(f"ğŸ² ç§å­èŒƒå›´: {self.combined_data['seed'].min()} - {self.combined_data['seed'].max()}")
        print(f"âœ… æˆåŠŸå®éªŒ: {self.combined_data['has_models'].sum()}/{len(self.combined_data)}")
        
        # æ€§èƒ½ç»Ÿè®¡
        score_columns = [col for col in self.combined_data.columns if col.endswith('_final')]
        
        if score_columns:
            print("\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
            
            for env in envs:
                env_data = self.combined_data[self.combined_data['environment'] == env]
                print(f"\nğŸ¯ {env}:")
                
                for score_col in score_columns:
                    if score_col in env_data.columns and not env_data[score_col].isna().all():
                        scores = env_data[score_col].dropna()
                        if len(scores) > 0:
                            score_name = score_col.replace('_final', '').upper()
                            print(f"  {score_name}:")
                            print(f"    å¹³å‡å€¼: {scores.mean():.2f} Â± {scores.std():.2f}")
                            print(f"    æœ€å¤§å€¼: {scores.max():.2f}")
                            print(f"    æœ€å°å€¼: {scores.min():.2f}")
                            print(f"    ä¸­ä½æ•°: {scores.median():.2f}")
                            
                            # ç½®ä¿¡åŒºé—´
                            if len(scores) > 1:
                                ci = stats.t.interval(0.95, len(scores)-1, 
                                                     loc=scores.mean(), 
                                                     scale=stats.sem(scores))
                                print(f"    95%ç½®ä¿¡åŒºé—´: [{ci[0]:.2f}, {ci[1]:.2f}]")
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        self.save_statistics_report()
    
    def save_statistics_report(self):
        """ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if self.combined_data is None:
            return
        
        report_file = self.base_dir / 'analysis_report.csv'
        self.combined_data.to_csv(report_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“„ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        summary_file = self.base_dir / 'summary_statistics.txt'
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("PDERL å¹¶è¡Œå®éªŒç»Ÿè®¡æ±‡æ€»\n")
            f.write("=" * 40 + "\n\n")
            
            envs = self.combined_data['environment'].unique()
            for env in envs:
                env_data = self.combined_data[self.combined_data['environment'] == env]
                f.write(f"ç¯å¢ƒ: {env}\n")
                f.write(f"å®éªŒæ•°é‡: {len(env_data)}\n")
                
                score_columns = [col for col in env_data.columns if col.endswith('_final')]
                for score_col in score_columns:
                    if score_col in env_data.columns and not env_data[score_col].isna().all():
                        scores = env_data[score_col].dropna()
                        if len(scores) > 0:
                            score_name = score_col.replace('_final', '')
                            f.write(f"{score_name}_å¹³å‡å€¼: {scores.mean():.2f}\n")
                            f.write(f"{score_name}_æ ‡å‡†å·®: {scores.std():.2f}\n")
                            f.write(f"{score_name}_æœ€å¤§å€¼: {scores.max():.2f}\n")
                
                f.write("\n")
        
        print(f"ğŸ“„ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜: {summary_file}")
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if self.combined_data is None or self.combined_data.empty:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œå¯è§†åŒ–")
            return
        
        print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('seaborn-v0_8')
        
        # 1. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
        self.plot_performance_distribution()
        
        # 2. ç§å­é—´æ€§èƒ½å¯¹æ¯”
        self.plot_seed_comparison()
        
        # 3. å­¦ä¹ æ›²çº¿å¯¹æ¯”
        self.plot_learning_curves()
        
        # 4. æ€§èƒ½ç›¸å…³æ€§åˆ†æ
        self.plot_performance_correlation()
        
        print("âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
    
    def plot_performance_distribution(self):
        """ç»˜åˆ¶æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾"""
        score_columns = [col for col in self.combined_data.columns if col.endswith('_final')]
        
        if not score_columns:
            return
        
        envs = self.combined_data['environment'].unique()
        
        fig, axes = plt.subplots(len(score_columns), len(envs), 
                                figsize=(4*len(envs), 4*len(score_columns)))
        
        if len(score_columns) == 1:
            axes = axes.reshape(1, -1)
        if len(envs) == 1:
            axes = axes.reshape(-1, 1)
        
        for i, score_col in enumerate(score_columns):
            for j, env in enumerate(envs):
                env_data = self.combined_data[self.combined_data['environment'] == env]
                scores = env_data[score_col].dropna()
                
                if len(scores) > 0:
                    ax = axes[i, j] if len(score_columns) > 1 else axes[j]
                    
                    # ç®±çº¿å›¾
                    bp = ax.boxplot(scores, patch_artist=True)
                    bp['boxes'][0].set_facecolor('lightblue')
                    
                    # æ•£ç‚¹å›¾
                    x = np.random.normal(1, 0.04, len(scores))
                    ax.scatter(x, scores, alpha=0.6, color='red', s=20)
                    
                    score_name = score_col.replace('_final', '').upper()
                    ax.set_title(f'{env}\n{score_name} æ€§èƒ½åˆ†å¸ƒ')
                    ax.set_ylabel('åˆ†æ•°')
                    ax.grid(True, alpha=0.3)
                    
                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    mean_val = scores.mean()
                    std_val = scores.std()
                    ax.text(0.02, 0.98, f'å‡å€¼: {mean_val:.1f}\næ ‡å‡†å·®: {std_val:.1f}', 
                           transform=ax.transAxes, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.base_dir / 'performance_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("ğŸ“Š æ€§èƒ½åˆ†å¸ƒå›¾å·²ä¿å­˜: performance_distribution.png")
    
    def plot_seed_comparison(self):
        """ç»˜åˆ¶ç§å­é—´æ€§èƒ½å¯¹æ¯”"""
        score_columns = [col for col in self.combined_data.columns if col.endswith('_final')]
        
        if not score_columns:
            return
        
        envs = self.combined_data['environment'].unique()
        
        for env in envs:
            env_data = self.combined_data[self.combined_data['environment'] == env]
            
            if len(env_data) < 2:
                continue
            
            fig, axes = plt.subplots(1, len(score_columns), 
                                   figsize=(6*len(score_columns), 5))
            
            if len(score_columns) == 1:
                axes = [axes]
            
            for i, score_col in enumerate(score_columns):
                scores = env_data[score_col].dropna()
                seeds = env_data.loc[scores.index, 'seed']
                
                if len(scores) > 0:
                    ax = axes[i]
                    
                    # æ¡å½¢å›¾
                    bars = ax.bar(range(len(scores)), scores, 
                                 color=plt.cm.viridis(np.linspace(0, 1, len(scores))))
                    
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for j, (bar, score) in enumerate(zip(bars, scores)):
                        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(scores),
                               f'{score:.1f}', ha='center', va='bottom')
                    
                    ax.set_xlabel('å®éªŒ (æŒ‰ç§å­æ’åº)')
                    ax.set_ylabel('åˆ†æ•°')
                    score_name = score_col.replace('_final', '').upper()
                    ax.set_title(f'{env} - {score_name} ç§å­å¯¹æ¯”')
                    ax.grid(True, alpha=0.3)
                    
                    # è®¾ç½®xè½´æ ‡ç­¾ä¸ºç§å­å€¼
                    ax.set_xticks(range(len(scores)))
                    ax.set_xticklabels([f'Seed {seed}' for seed in seeds], rotation=45)
                    
                    # æ·»åŠ å¹³å‡çº¿
                    mean_val = scores.mean()
                    ax.axhline(y=mean_val, color='red', linestyle='--', alpha=0.7, 
                              label=f'å¹³å‡å€¼: {mean_val:.1f}')
                    ax.legend()
            
            plt.tight_layout()
            plt.savefig(self.base_dir / f'seed_comparison_{env.lower()}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“Š ç§å­å¯¹æ¯”å›¾å·²ä¿å­˜: seed_comparison_{env.lower()}.png")
    
    def plot_learning_curves(self):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿å¯¹æ¯”"""
        envs = self.combined_data['environment'].unique()
        
        for env in envs:
            env_experiments = {name: data for name, data in self.experiments.items() 
                             if data['env_name'] == env}
            
            if len(env_experiments) < 2:
                continue
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            score_types = ['erl_score', 'ddpg_score']
            colors = plt.cm.tab10(np.linspace(0, 1, len(env_experiments)))
            
            for i, score_type in enumerate(score_types):
                ax = axes[i]
                
                for j, (exp_name, exp_data) in enumerate(env_experiments.items()):
                    if score_type in exp_data and not exp_data[score_type].empty:
                        df = exp_data[score_type]
                        if 'fitness' in df.columns:
                            # å¹³æ»‘æ›²çº¿
                            window = max(1, len(df) // 20)
                            smoothed = df['fitness'].rolling(window=window, center=True).mean()
                            
                            ax.plot(smoothed, color=colors[j], alpha=0.7, 
                                   label=f'Seed {exp_data["seed"]}')
                
                ax.set_xlabel('è®­ç»ƒè½®æ¬¡')
                ax.set_ylabel('é€‚åº”åº¦åˆ†æ•°')
                ax.set_title(f'{env} - {score_type.upper()} å­¦ä¹ æ›²çº¿')
                ax.grid(True, alpha=0.3)
                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            
            plt.tight_layout()
            plt.savefig(self.base_dir / f'learning_curves_{env.lower()}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            print(f"ğŸ“ˆ å­¦ä¹ æ›²çº¿å›¾å·²ä¿å­˜: learning_curves_{env.lower()}.png")
    
    def plot_performance_correlation(self):
        """ç»˜åˆ¶æ€§èƒ½ç›¸å…³æ€§åˆ†æ"""
        score_columns = [col for col in self.combined_data.columns 
                        if col.endswith('_final') or col.endswith('_max') or col.endswith('_mean')]
        
        if len(score_columns) < 2:
            return
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        corr_data = self.combined_data[score_columns].corr()
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        mask = np.triu(np.ones_like(corr_data, dtype=bool))
        
        sns.heatmap(corr_data, mask=mask, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})
        
        plt.title('æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ')
        plt.tight_layout()
        plt.savefig(self.base_dir / 'performance_correlation.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("ğŸ“Š ç›¸å…³æ€§åˆ†æå›¾å·²ä¿å­˜: performance_correlation.png")
    
    def generate_best_models_report(self):
        """ç”Ÿæˆæœ€ä½³æ¨¡å‹æŠ¥å‘Š"""
        if self.combined_data is None or self.combined_data.empty:
            return
        
        print("\nğŸ† æœ€ä½³æ¨¡å‹æŠ¥å‘Š")
        print("=" * 40)
        
        envs = self.combined_data['environment'].unique()
        best_models = {}
        
        for env in envs:
            env_data = self.combined_data[self.combined_data['environment'] == env]
            
            # æ‰¾åˆ°ERLåˆ†æ•°æœ€é«˜çš„å®éªŒ
            if 'erl_score_final' in env_data.columns:
                best_idx = env_data['erl_score_final'].idxmax()
                if not pd.isna(best_idx):
                    best_exp = env_data.loc[best_idx]
                    best_models[env] = {
                        'experiment': best_exp['experiment'],
                        'seed': best_exp['seed'],
                        'erl_score': best_exp['erl_score_final'],
                        'ddpg_score': best_exp.get('ddpg_score_final', 'N/A')
                    }
                    
                    print(f"ğŸ¯ {env}:")
                    print(f"  æœ€ä½³å®éªŒ: {best_exp['experiment']}")
                    print(f"  ç§å­: {best_exp['seed']}")
                    print(f"  ERLåˆ†æ•°: {best_exp['erl_score_final']:.2f}")
                    if 'ddpg_score_final' in best_exp:
                        print(f"  DDPGåˆ†æ•°: {best_exp['ddpg_score_final']:.2f}")
                    
                    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                    exp_data = self.experiments[best_exp['experiment']]
                    if exp_data['model_files']:
                        print(f"  æ¨¡å‹æ–‡ä»¶: {len(exp_data['model_files'])} ä¸ª")
                        for model_file in exp_data['model_files']:
                            print(f"    ğŸ“„ {Path(model_file).name}")
                    print()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ä¿¡æ¯
        best_models_file = self.base_dir / 'best_models.json'
        with open(best_models_file, 'w', encoding='utf-8') as f:
            json.dump(best_models, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æœ€ä½³æ¨¡å‹æŠ¥å‘Šå·²ä¿å­˜: {best_models_file}")
        
        return best_models

def main():
    parser = argparse.ArgumentParser(description='PDERL å¹¶è¡Œå®éªŒç»“æœåˆ†æ')
    parser.add_argument('-dir', '--directory', required=True,
                       help='å¹¶è¡Œå®éªŒç»“æœç›®å½•')
    parser.add_argument('--no-plots', action='store_true',
                       help='ä¸ç”Ÿæˆå›¾è¡¨')
    parser.add_argument('--stats-only', action='store_true',
                       help='ä»…ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ParallelResultsAnalyzer(args.directory)
    
    # åŠ è½½æ•°æ®
    if not analyzer.load_experiment_data():
        print("âŒ æ— æ³•åŠ è½½å®éªŒæ•°æ®")
        return
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    analyzer.generate_statistics_report()
    
    # ç”Ÿæˆæœ€ä½³æ¨¡å‹æŠ¥å‘Š
    analyzer.generate_best_models_report()
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if not args.stats_only and not args.no_plots:
        try:
            analyzer.create_visualizations()
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {str(e)}")
            print("ğŸ’¡ å¯ä»¥ä½¿ç”¨ --no-plots å‚æ•°è·³è¿‡å›¾è¡¨ç”Ÿæˆ")
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {analyzer.base_dir}")

if __name__ == '__main__':
    main()