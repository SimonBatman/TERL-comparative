import os
import torch
from torch.utils.tensorboard import SummaryWriter
import numpy as np


class TensorBoardTracker:
    """TensorBoardæ—¥å¿—è®°å½•å™¨ï¼Œæ›¿ä»£CSVæ–‡ä»¶å­˜å‚¨"""
    
    def __init__(self, parameters, log_dir=None):
        """
        åˆå§‹åŒ–TensorBoardè®°å½•å™¨
        
        Args:
            parameters: å‚æ•°å¯¹è±¡
            log_dir: TensorBoardæ—¥å¿—ç›®å½•ï¼Œå¦‚æžœä¸ºNoneåˆ™ä½¿ç”¨parameters.save_foldername
        """
        self.parameters = parameters
        
        # è®¾ç½®æ—¥å¿—ç›®å½•
        if log_dir is None:
            self.log_dir = os.path.join(parameters.save_foldername, 'tensorboard_logs')
        else:
            self.log_dir = log_dir
            
        # åˆ›å»ºç›®å½•
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–SummaryWriter
        self.writer = SummaryWriter(log_dir=self.log_dir)
        
        # è®°å½•å‚æ•°ä¿¡æ¯
        self._log_hyperparameters()
        
        print(f"ðŸ“Š TensorBoardæ—¥å¿—å·²å¯ç”¨: {self.log_dir}")
        print(f"ðŸ’¡ æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹: tensorboard --logdir {self.log_dir}")
    
    def _log_hyperparameters(self):
        """è®°å½•è¶…å‚æ•°"""
        hparams = {
            'env_name': self.parameters.env_name,
            'seed': self.parameters.seed,
            'pop_size': self.parameters.pop_size,
            'num_frames': self.parameters.num_frames,
            'batch_size': self.parameters.batch_size,
            'buffer_size': self.parameters.buffer_size,
            'tau': self.parameters.tau,
            'gamma': self.parameters.gamma,
            'mutation_mag': self.parameters.mutation_mag,
        }
        
        # æ·»åŠ å¸ƒå°”å‚æ•°
        bool_params = ['use_cuda', 'novelty', 'proximal_mut', 'distil', 'per']
        for param in bool_params:
            if hasattr(self.parameters, param):
                hparams[param] = getattr(self.parameters, param)
        
        # è®°å½•è¶…å‚æ•°
        self.writer.add_hparams(hparams, {})
    
    def log_training_step(self, step, metrics):
        """
        è®°å½•è®­ç»ƒæ­¥éª¤çš„æŒ‡æ ‡
        
        Args:
            step: è®­ç»ƒæ­¥æ•°ï¼ˆå¸§æ•°æˆ–æ¸¸æˆæ•°ï¼‰
            metrics: æŒ‡æ ‡å­—å…¸
        """
        for metric_name, value in metrics.items():
            if value is not None:
                self.writer.add_scalar(f'Training/{metric_name}', value, step)
    
    def log_performance(self, step, erl_score, ddpg_reward, best_train_fitness=None):
        """
        è®°å½•æ€§èƒ½æŒ‡æ ‡
        
        Args:
            step: æ­¥æ•°
            erl_score: ERLæµ‹è¯•åˆ†æ•°
            ddpg_reward: DDPGå¥–åŠ±
            best_train_fitness: æœ€ä½³è®­ç»ƒé€‚åº”åº¦
        """
        if erl_score is not None:
            self.writer.add_scalar('Performance/ERL_Test_Score', erl_score, step)
        
        if ddpg_reward is not None:
            self.writer.add_scalar('Performance/DDPG_Reward', ddpg_reward, step)
        
        if best_train_fitness is not None:
            self.writer.add_scalar('Performance/Best_Train_Fitness', best_train_fitness, step)
    
    def log_losses(self, step, pg_loss=None, bc_loss=None, critic_loss=None):
        """
        è®°å½•æŸå¤±å‡½æ•°
        
        Args:
            step: æ­¥æ•°
            pg_loss: ç­–ç•¥æ¢¯åº¦æŸå¤±
            bc_loss: è¡Œä¸ºå…‹éš†æŸå¤±
            critic_loss: è¯„è®ºå®¶æŸå¤±
        """
        if pg_loss is not None:
            self.writer.add_scalar('Losses/Policy_Gradient_Loss', pg_loss, step)
        
        if bc_loss is not None:
            self.writer.add_scalar('Losses/Behavior_Cloning_Loss', bc_loss, step)
        
        if critic_loss is not None:
            self.writer.add_scalar('Losses/Critic_Loss', critic_loss, step)
    
    def log_evolution_stats(self, step, elite_ratio, selected_ratio, discarded_ratio, pop_novelty=None):
        """
        è®°å½•è¿›åŒ–ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            step: æ­¥æ•°
            elite_ratio: ç²¾è‹±æ¯”ä¾‹
            selected_ratio: é€‰æ‹©æ¯”ä¾‹
            discarded_ratio: ä¸¢å¼ƒæ¯”ä¾‹
            pop_novelty: ç§ç¾¤æ–°é¢–æ€§
        """
        self.writer.add_scalar('Evolution/Elite_Ratio', elite_ratio, step)
        self.writer.add_scalar('Evolution/Selected_Ratio', selected_ratio, step)
        self.writer.add_scalar('Evolution/Discarded_Ratio', discarded_ratio, step)
        
        if pop_novelty is not None:
            self.writer.add_scalar('Evolution/Population_Novelty', pop_novelty, step)
    
    def log_network_weights(self, step, actor_net, critic_net=None):
        """
        è®°å½•ç½‘ç»œæƒé‡åˆ†å¸ƒ
        
        Args:
            step: æ­¥æ•°
            actor_net: Actorç½‘ç»œ
            critic_net: Criticç½‘ç»œ
        """
        # è®°å½•Actorç½‘ç»œæƒé‡
        for name, param in actor_net.named_parameters():
            if param.grad is not None:
                self.writer.add_histogram(f'Actor_Weights/{name}', param.data, step)
                self.writer.add_histogram(f'Actor_Gradients/{name}', param.grad.data, step)
        
        # è®°å½•Criticç½‘ç»œæƒé‡
        if critic_net is not None:
            for name, param in critic_net.named_parameters():
                if param.grad is not None:
                    self.writer.add_histogram(f'Critic_Weights/{name}', param.data, step)
                    self.writer.add_histogram(f'Critic_Gradients/{name}', param.grad.data, step)
    
    def log_episode_rewards(self, step, rewards):
        """
        è®°å½•å›žåˆå¥–åŠ±åˆ†å¸ƒ
        
        Args:
            step: æ­¥æ•°
            rewards: å¥–åŠ±åˆ—è¡¨
        """
        if len(rewards) > 0:
            self.writer.add_histogram('Episode/Reward_Distribution', np.array(rewards), step)
            self.writer.add_scalar('Episode/Mean_Reward', np.mean(rewards), step)
            self.writer.add_scalar('Episode/Max_Reward', np.max(rewards), step)
            self.writer.add_scalar('Episode/Min_Reward', np.min(rewards), step)
    
    def log_custom_metric(self, metric_name, value, step, category='Custom'):
        """
        è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡
        
        Args:
            metric_name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            step: æ­¥æ•°
            category: åˆ†ç±»åç§°
        """
        self.writer.add_scalar(f'{category}/{metric_name}', value, step)
    
    def log_text(self, tag, text, step):
        """
        è®°å½•æ–‡æœ¬ä¿¡æ¯
        
        Args:
            tag: æ ‡ç­¾
            text: æ–‡æœ¬å†…å®¹
            step: æ­¥æ•°
        """
        self.writer.add_text(tag, text, step)
    
    def close(self):
        """å…³é—­TensorBoard writer"""
        if hasattr(self, 'writer'):
            self.writer.close()
            print(f"ðŸ“Š TensorBoardæ—¥å¿—å·²ä¿å­˜: {self.log_dir}")
    
    def __del__(self):
        """æžæž„å‡½æ•°ï¼Œç¡®ä¿writerè¢«æ­£ç¡®å…³é—­"""
        self.close()


class LegacyCSVTracker:
    """ä¿æŒå‘åŽå…¼å®¹çš„CSVè®°å½•å™¨"""
    
    def __init__(self, parameters, vars_string, project_string):
        self.vars_string = vars_string
        self.project_string = project_string
        self.foldername = parameters.save_foldername
        self.all_tracker = [[[],0.0,[]] for _ in vars_string]
        self.counter = 0
        self.conv_size = 10
        
        if not os.path.exists(self.foldername):
            os.makedirs(self.foldername)
    
    def update(self, updates, generation):
        """ä¿æŒåŽŸæœ‰çš„CSVæ›´æ–°é€»è¾‘"""
        self.counter += 1
        for update, var in zip(updates, self.all_tracker):
            if update == None: continue
            var[0].append(update)

        # Constrain size of convolution
        for var in self.all_tracker:
            if len(var[0]) > self.conv_size: var[0].pop(0)

        # Update new average
        for var in self.all_tracker:
            if len(var[0]) == 0: continue
            var[1] = sum(var[0])/float(len(var[0]))

        if self.counter % 4 == 0:  # Save to csv file
            for i, var in enumerate(self.all_tracker):
                if len(var[0]) == 0: continue
                var[2].append(np.array([generation, var[1]]))
                filename = os.path.join(self.foldername, self.vars_string[i] + self.project_string)
                try:
                    np.savetxt(filename, np.array(var[2]), fmt='%.3f', delimiter=',')
                except:
                    print('Failed to save progress')